# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q0VFqfltp9q7kwtfZ7m3z5T_CYFUlYBj
"""

import sys
import numpy as np
import matplotlib

print("Python: ", sys.version)
print("Numpy: ", np.__version__)
print("Matplotlib: ", matplotlib)

# 1 neuron with 3 inputs

x = [2.5, 4.6, 7.1]
weights = [2.2, 5.4, 7.8]
b = 3


output = x[0]*weights[0] + x[1]*weights[1] + x[2]*weights[2] + b
print(output)

# 1 neuron with 4 inputs
inputs = [0, 5, 3, 5.5]
weights = [0.7, 0.3, -0.2, 2.0]
bias = 1


output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias
print(output)

# whole Layer
inputs = [0, 3, 3, 3]
weights1 = [0.6, 0.7, -0.34, 3.46]
weights2 = [0.26, -0.87, 0.45, -0.27]
weights3 = [-0.73, -0.67, 0.28, 0.9]

bias1 = 1
bias2 = 2
bias3 = 3

output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,
          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,
          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3]
print(output)

# A cleaner way
inputs = [1, 2, 3, 2.5]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]

layer_outputs = []      # output of the current layer
for neuron_weights, neuron_bias in zip(weights, biases):
  neuron_output = 0     # output of the given neuron
  for n_input, weight in zip(inputs, neuron_weights):
    neuron_output += n_input * weight
  neuron_output += neuron_bias
  layer_outputs.append(neuron_output)

print(layer_outputs)

import numpy as np

inputs = [1, 2, 3, 4]
weights = [1.2, 3.8, -6.5, 1.0]
bias = 2

output = np.dot(weights, inputs) + bias    # Order of weights and inputs in np.dot does not matter in this
print(output)

inputs = [1, 2, 3, 4]

weights = [[0.325, 0.856, -0.765, 2.674],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.27, 0.87]]

biases = [1, 2, 3]

output = np.dot(weights, inputs) + biases   # Order of weights and inputs matter in this
print(output)

import numpy as np

inputs = [[1, 2, 3, 2.5],
          [2.0, 5.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]


weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]

output = np.dot(inputs, np.array(weights).T) + biases   # Order of weights and inputs matter in this
print(output)

import numpy as np

inputs = [[1, 2, 3, 2.5],
          [2.0, 5.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]


weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]


weights2 = [[0.1, -0.14, 0.5],
            [-0.5, 0.12, -0.33],
            [-0.44, 0.73, -0.13]]

biases2 = [-1, 2, -0.5]

layer1_output = np.dot(inputs, np.array(weights).T) + biases   
layer2_output = np.dot(layer1_output, np.array(weights2).T) + biases2
print(layer2_output)

import numpy as np
np.random.seed(0)

X = [[1, 3, 3.5, 4],
          [2.0, 5.0, -7.0, 2.0],
          [-1.5, 5.7, 3.3, -3.8]]


class Layer_Dense:
  def __init__(self, n_inputs, n_neurons):

    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)
    self.biases = np.zeros((1, n_neurons))
  def forward(self, inputs):
    self.output = np.dot(inputs, self.weights) + self.biases

layer1 = Layer_Dense(4, 5)      # 4 is the input size, 5 is the no of neurons(could be any number)
layer2 = Layer_Dense(5, 2)      # 5 is the number of neuron(output of layer 1), and 2 is the number of the output neuron

layer1.forward(X)
print(layer1.output)
layer2.forward(layer1.output)
print(layer2.output)





# relu activation
import numpy as np
np.random.seed(0)

X = [[1, 2, 3, 2.5],
          [2.0, 5.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]

inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]
output = []

for i in inputs:
  if i > 0:
    output.append(i)
  elif i <= 0:
    output.append(0)

print(output)

# Softmax Activation
inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]

exponential_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))
outputs = exponential_values / np.sum(exponential_values, axis=1, keepdims=True)

# categorical cross_entropy_loss


layer_output = [0.9, 0.05, 0.1]
target = [1, 0, 0]


ccel = -(math.log(layer_output[0]) * target_[0] +
         math.log(layer_output[1]) * target_[1] +
         math.log(layer_output[2]) * target_[2])


print(ccel)

